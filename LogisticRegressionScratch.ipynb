{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML from Scratch-Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to real-world machine learning, around 70% of the problems are classification-based, where, on the basis of the available set of features, your model tries to predict that out of a given set of categories(discrete possible outcomes), what category does your target variable might belong to. \n",
    "\n",
    "**Today, in this article, we are going to have a look at Multinomial Logistic Regressionâˆ’ one of the classic supervised machine learning algorithms capable of doing multi-class classification, i.e., predict an outcome for the target variable when there are more than 2 possible discrete classes of outcomes.\n",
    "This is a project-based guide, where we will see how to code an MLR model from scratch while understanding the mathematics involved that allows the model to make predictions.**\n",
    "\n",
    "For the project, we will be working on the famous UCI Cleveland Heart Disease dataset. We will create an ML model from scratch that uses multinomial logistic regression, capable of predicting the severity of heart disease within a patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Project Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will import the dataset. According to the <a href=\"https://archive.ics.uci.edu/ml/datasets/Heart+Disease\">data source</a>, the dataset does not have column names. So we will set the header attribute as None and then we will manually set the column names as per the information available on the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca thal  num  \n",
       "0    3.0  0.0  6.0    0  \n",
       "1    2.0  3.0  3.0    2  \n",
       "2    2.0  2.0  7.0    1  \n",
       "3    3.0  0.0  3.0    0  \n",
       "4    1.0  0.0  3.0    0  "
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'] #column names\n",
    "\n",
    "df = pd.read_csv('processed.cleveland.data.csv', header = None)\n",
    "df.columns = col_names # setting dataframe column names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually start working on the model, let us first do preprocessing on our data. The first step is cheching if the dataset has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.0    19\n",
      "57.0    17\n",
      "54.0    16\n",
      "59.0    14\n",
      "52.0    13\n",
      "60.0    12\n",
      "51.0    12\n",
      "56.0    11\n",
      "62.0    11\n",
      "44.0    11\n",
      "41.0    10\n",
      "64.0    10\n",
      "67.0     9\n",
      "63.0     9\n",
      "42.0     8\n",
      "43.0     8\n",
      "53.0     8\n",
      "65.0     8\n",
      "55.0     8\n",
      "61.0     8\n",
      "45.0     8\n",
      "46.0     7\n",
      "66.0     7\n",
      "50.0     7\n",
      "48.0     7\n",
      "47.0     5\n",
      "49.0     5\n",
      "39.0     4\n",
      "68.0     4\n",
      "35.0     4\n",
      "70.0     4\n",
      "69.0     3\n",
      "40.0     3\n",
      "71.0     3\n",
      "34.0     2\n",
      "37.0     2\n",
      "38.0     2\n",
      "74.0     1\n",
      "29.0     1\n",
      "77.0     1\n",
      "76.0     1\n",
      "Name: age, dtype: int64\n",
      "1.0    206\n",
      "0.0     97\n",
      "Name: sex, dtype: int64\n",
      "4.0    144\n",
      "3.0     86\n",
      "2.0     50\n",
      "1.0     23\n",
      "Name: cp, dtype: int64\n",
      "120.0    37\n",
      "130.0    36\n",
      "140.0    32\n",
      "110.0    19\n",
      "150.0    17\n",
      "138.0    12\n",
      "128.0    12\n",
      "125.0    11\n",
      "160.0    11\n",
      "112.0     9\n",
      "132.0     8\n",
      "118.0     7\n",
      "124.0     6\n",
      "135.0     6\n",
      "108.0     6\n",
      "152.0     5\n",
      "134.0     5\n",
      "145.0     5\n",
      "100.0     4\n",
      "170.0     4\n",
      "122.0     4\n",
      "136.0     3\n",
      "105.0     3\n",
      "115.0     3\n",
      "142.0     3\n",
      "126.0     3\n",
      "180.0     3\n",
      "178.0     2\n",
      "94.0      2\n",
      "148.0     2\n",
      "146.0     2\n",
      "102.0     2\n",
      "144.0     2\n",
      "172.0     1\n",
      "117.0     1\n",
      "106.0     1\n",
      "156.0     1\n",
      "123.0     1\n",
      "154.0     1\n",
      "200.0     1\n",
      "129.0     1\n",
      "192.0     1\n",
      "158.0     1\n",
      "174.0     1\n",
      "155.0     1\n",
      "104.0     1\n",
      "101.0     1\n",
      "114.0     1\n",
      "165.0     1\n",
      "164.0     1\n",
      "Name: trestbps, dtype: int64\n",
      "234.0    6\n",
      "204.0    6\n",
      "197.0    6\n",
      "269.0    5\n",
      "212.0    5\n",
      "        ..\n",
      "360.0    1\n",
      "237.0    1\n",
      "178.0    1\n",
      "306.0    1\n",
      "311.0    1\n",
      "Name: chol, Length: 152, dtype: int64\n",
      "0.0    258\n",
      "1.0     45\n",
      "Name: fbs, dtype: int64\n",
      "0.0    151\n",
      "2.0    148\n",
      "1.0      4\n",
      "Name: restecg, dtype: int64\n",
      "162.0    11\n",
      "160.0     9\n",
      "163.0     9\n",
      "152.0     8\n",
      "150.0     7\n",
      "         ..\n",
      "134.0     1\n",
      "177.0     1\n",
      "128.0     1\n",
      "99.0      1\n",
      "90.0      1\n",
      "Name: thalach, Length: 91, dtype: int64\n",
      "0.0    204\n",
      "1.0     99\n",
      "Name: exang, dtype: int64\n",
      "0.0    99\n",
      "1.2    17\n",
      "1.0    14\n",
      "0.6    14\n",
      "0.8    13\n",
      "1.4    13\n",
      "0.2    12\n",
      "1.6    11\n",
      "1.8    10\n",
      "2.0     9\n",
      "0.4     9\n",
      "0.1     7\n",
      "2.8     6\n",
      "2.6     6\n",
      "0.5     5\n",
      "3.0     5\n",
      "1.5     5\n",
      "1.9     5\n",
      "2.2     4\n",
      "3.6     4\n",
      "3.4     3\n",
      "0.3     3\n",
      "2.4     3\n",
      "0.9     3\n",
      "4.0     3\n",
      "4.2     2\n",
      "2.3     2\n",
      "3.2     2\n",
      "2.5     2\n",
      "1.1     2\n",
      "3.5     1\n",
      "1.3     1\n",
      "5.6     1\n",
      "0.7     1\n",
      "2.9     1\n",
      "3.1     1\n",
      "3.8     1\n",
      "2.1     1\n",
      "6.2     1\n",
      "4.4     1\n",
      "Name: oldpeak, dtype: int64\n",
      "1.0    142\n",
      "2.0    140\n",
      "3.0     21\n",
      "Name: slope, dtype: int64\n",
      "0.0    176\n",
      "1.0     65\n",
      "2.0     38\n",
      "3.0     20\n",
      "?        4\n",
      "Name: ca, dtype: int64\n",
      "3.0    166\n",
      "7.0    117\n",
      "6.0     18\n",
      "?        2\n",
      "Name: thal, dtype: int64\n",
      "0    164\n",
      "1     55\n",
      "2     36\n",
      "3     35\n",
      "4     13\n",
      "Name: num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(df[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the columns **ca** and **thal** have 4 and 2 **'?'** values respectively. These are null values that we need to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        303 non-null    float64\n",
      " 12  thal      303 non-null    float64\n",
      " 13  num       303 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.replace({'?': np.nan}, inplace = True) # converting '?' to NaN values\n",
    "df[['ca', 'thal']] = df[['ca', 'thal']].astype('float64') # Casting columns data-type to floats\n",
    "df['ca'].replace({np.nan: df['ca'].median()}, inplace = True) # replaces null values of ca column with median value\n",
    "df['thal'].replace({np.nan: df['thal'].median()}, inplace = True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned our data, let us have a look at the statistical analysis of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.438944</td>\n",
       "      <td>0.679868</td>\n",
       "      <td>3.158416</td>\n",
       "      <td>131.689769</td>\n",
       "      <td>246.693069</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>149.607261</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.600660</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>4.722772</td>\n",
       "      <td>0.937294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.038662</td>\n",
       "      <td>0.467299</td>\n",
       "      <td>0.960126</td>\n",
       "      <td>17.599748</td>\n",
       "      <td>51.776918</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.994971</td>\n",
       "      <td>22.875003</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>0.934375</td>\n",
       "      <td>1.938383</td>\n",
       "      <td>1.228536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
       "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
       "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
       "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
       "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
       "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
       "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.663366   \n",
       "std      0.994971   22.875003    0.469794    1.161075    0.616226    0.934375   \n",
       "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000   \n",
       "75%      2.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    3.000000    3.000000   \n",
       "\n",
       "             thal         num  \n",
       "count  303.000000  303.000000  \n",
       "mean     4.722772    0.937294  \n",
       "std      1.938383    1.228536  \n",
       "min      3.000000    0.000000  \n",
       "25%      3.000000    0.000000  \n",
       "50%      3.000000    0.000000  \n",
       "75%      7.000000    2.000000  \n",
       "max      7.000000    4.000000  "
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon observing the data, we can see that the data needs to be scaled since we have values in the range (1, 1e+2). The main reason we are scaling our data is that since we will be using Stochastic Gradient Descent for optimizing our model parameters, scaling can significantly improve the speed and accuracy of our optimizer.\n",
    "\n",
    "Here, we will use standard scaling in order to standardize the data.\n",
    "\n",
    "The first step is to split the dataset into target and feature arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), 303)"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all the features within our dataset\n",
    "features = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']] \n",
    "features = features.to_numpy() # converts feature set to numpy array\n",
    "target = df['num'].to_numpy() # converts target column to numpy array\n",
    "features.shape, len(target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our standard scaling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for standardizing data\n",
    "def standardScaler(feature_array):\n",
    "    \"\"\"Takes the numpy.ndarray object containing the features and performs standardization on the matrix.\n",
    "    The function iterates through each column and performs scaling on them individually.\n",
    "    \n",
    "    Args-\n",
    "        feature_array- Numpy array containing training features\n",
    "    \n",
    "    Returns-\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    total_cols = feature_array.shape[1] # total number of columns \n",
    "    for i in range(total_cols): # iterating through each column\n",
    "        feature_col = feature_array[:, i]\n",
    "        mean = feature_col.mean() # mean stores mean value for the column\n",
    "        std = feature_col.std() # std stores standard deviation value for the column\n",
    "        feature_array[:, i] = (feature_array[:, i] - mean) / std # standard scaling of each element of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "standardScaler(features) # performing standardization on our feature set \n",
    "\n",
    "# checking if standardization worked\n",
    "total_cols = features.shape[1] # total number of columns \n",
    "for i in range(total_cols):\n",
    "    print(features[:, i].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed the data wrangling process, let us start working on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating the Model fromÂ Scratch\n",
    "\n",
    "The multinomial regression function consists of two functional layers-\n",
    "1. Linear prediction function (a.k.a. logit layer)\n",
    "2. Softmax function (a.k.a. softmax layer)\n",
    "\n",
    "Let us implement these two function in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create randomized weights and biases for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating randomized weights for our linear predictor func\n",
    "weights = np.random.rand(5, 13)\n",
    "# creating randomized biases for our linear predictor func\n",
    "biases = np.random.rand(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearPredict(featureMat, weights, biases):\n",
    "    \"\"\"This is the linear predictor function for out MLR model. It calculates the logit scores for each possible outcome.\n",
    "    \n",
    "    Args-\n",
    "        featureMat- A numpy array of features\n",
    "        weights- A numpy array of weights for our model\n",
    "        biases- A numpy array of biases for our model\n",
    "    \n",
    "    Returns-\n",
    "        logitScores- Logit scores for each possible outcome of the target variable for each feature set in the feature matrix\n",
    "    \"\"\"\n",
    "    logitScores = np.array([np.empty([5]) for i in range(featureMat.shape[0])]) # creating empty(garbage value) array for each feature set\n",
    "    \n",
    "    for i in range(featureMat.shape[0]): # iterating through each feature set\n",
    "        logitScores[i] = (weights.dot(featureMat[i].reshape(-1,1)) + biases).reshape(-1) # calculates logit score for each feature set then flattens the logit vector \n",
    "    \n",
    "    return logitScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test the function for our feature matrix. The final output should be a 303 x 5 matrix since we have 303 feature sets and 5 target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 5)"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresTest = df[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']]\n",
    "featuresTest = featuresTest.to_numpy() # converts feature set to numpy array\n",
    "logitTest = linearPredict(featuresTest, weights, biases)\n",
    "logitTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([311.51259603, 270.623595  , 320.84475682, 519.22643806,\n",
       "       301.99298163])"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitTest[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the function worked just fine. Now to the next step, converting logit scores to probability values. Here's where the softmax function comes into picture. \n",
    "\n",
    "What the softmax function does is that it normalizes the logit scores for each possible outcome in a way such that the normalized outputs follow a probabilistic distribution.Â \n",
    "In layman's terms, the softmax function converts logit scores of the possible outcomes of a feature set to probability values.\n",
    "\n",
    "Now, let us define the softmax function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxNormalizer(logitMatrix):\n",
    "    \"\"\"Converts logit scores for each possible outcome to probability values.\n",
    "    \n",
    "    Args-\n",
    "        logitMatrix - This is the output of our logitPredict function; consists  logit scores for each feature set\n",
    "    \n",
    "    Returns-\n",
    "        probabilities - Probability value of each outcome for each feature set\n",
    "    \"\"\"\n",
    "    \n",
    "    probabilities = np.array([np.empty([5]) for i in range(logitMatrix.shape[0])]) # creating empty(garbage value) array for each feature set\n",
    "\n",
    "    for i in range(logitMatrix.shape[0]):\n",
    "        exp = np.exp(logitMatrix[i]) # exponentiates each element of the logit array\n",
    "        sumOfArr = np.sum(exp) # adds up all the values in the exponentiated array\n",
    "        probabilities[i] = exp/sumOfArr # logit scores to probability values\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our softmax function as well, let us combine these two functions into a single multinomial function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialLogReg(features, weights, biases):\n",
    "    \"\"\"Performs logistic regression on a given feature set.\n",
    "    \n",
    "    Args- \n",
    "        features- Numpy array of features(standardized)\n",
    "        weights- A numpy array of weights for our model\n",
    "        biases- A numpy array of biases for our model\n",
    "    \n",
    "    Returns-\n",
    "        probabilities, predictions\n",
    "        Here,\n",
    "            probabilities: Probability values for each possible outcome for each feature set in the feature matrix\n",
    "            predictions: Outcome with max probability for each feature set\n",
    "    \"\"\"\n",
    "    logitScores = linearPredict(features, weights, biases)\n",
    "    probabilities = softmaxNormalizer(logitScores)\n",
    "    predictions = np.array([np.argmax(i) for i in probabilities]) #returns the outcome with max probability\n",
    "    return probabilities, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us perform logistic regression on our feature set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 5)\n",
      "[3 3 0 2 1 1 2 4 0 2 2 3 1 1 1 4 2 4 1 1 0 3 0 3 0 2 3 3 4 0 3 2 3 2 4 1 0\n",
      " 3 0 1 3 1 3 1 3 0 1 0 3 2 1 1 1 1 0 0 2 1 2 0 2 4 0 1 2 0 0 3 0 4 3 2 0 3\n",
      " 1 3 0 3 1 0 0 2 1 3 1 1 1 3 3 3 0 0 3 2 3 1 0 3 1 1 1 1 3 3 2 1 4 3 0 2 3\n",
      " 2 1 2 3 1 1 2 3 0 1 3 4 0 3 3 3 0 1 2 3 1 1 4 2 3 0 0 4 2 4 3 1 0 0 1 3 1\n",
      " 1 2 1 2 3 3 0 0 4 3 3 3 1 3 2 0 1 4 1 1 4 1 0 4 4 3 3 0 2 0 1 1 2 3 3 3 3\n",
      " 3 2 3 3 3 1 0 1 2 0 0 3 4 1 3 1 3 1 3 1 3 0 4 1 4 1 4 4 2 1 3 1 4 2 3 1 1\n",
      " 1 0 2 1 1 3 0 0 2 3 1 3 3 0 0 1 2 1 1 1 2 3 1 0 1 0 2 1 4 2 0 3 2 2 2 3 3\n",
      " 1 2 3 3 1 0 4 1 2 1 1 0 3 2 2 3 3 3 2 3 2 0 1 2 1 3 0 2 1 3 2 3 3 4 0 4 1\n",
      " 2 2 1 2 4 3 1]\n"
     ]
    }
   ],
   "source": [
    "probabilities, predictions = multinomialLogReg(features, weights, biases)\n",
    "print(probabilities.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now check the accuracy of our model. Since the weights and biases were randomly generated, we can't expect our model to be very accurate at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.881188118811881\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, target):\n",
    "    \"\"\"Calculates total accuracy for our model.\n",
    "    \n",
    "    Args- \n",
    "        predictions- Predicted target outcomes as predicted by our MLR function\n",
    "        target- Actual target values\n",
    "    \n",
    "    Returns-\n",
    "        accuracy- Accuracy percentage of our model\n",
    "    \"\"\"\n",
    "    correctPred = 0\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(n):\n",
    "        if predictions[int(i)] == target[int(i)]:\n",
    "            correctPred += 1\n",
    "    accuracy = correctPred/n * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = accuracy(predictions, target) #calculating accuracy for our model\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the initial model accuracy is only about 17%, which is very poor to even consider this model for making any heart disease predictions in real life.Â \n",
    "\n",
    "As a result,  we need to optimize our model parameters in order to improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization\n",
    "\n",
    "In this section, we will be working on optimizing our MLR model.\n",
    "\n",
    "But before we move on towards optimization, let us first split our model into separate training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255, 13), (255,), (52, 13), (52,))"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_split(dataframe, test_size = 0.2):\n",
    "    \"\"\"Splits dataset into training and testing sets.\n",
    "    \n",
    "    Args- \n",
    "        dataframe- The dataframe object you want to split\n",
    "        test_size- Size of test dataset that you want\n",
    "    \n",
    "    Returns-\n",
    "        train_features, train_target, test_features, test_target \n",
    "    \"\"\"\n",
    "    \n",
    "    data = dataframe.to_numpy() # converts dataframe to numpy array\n",
    "    totalRows = data.shape[0] # total rows in the dataset\n",
    "    testRows = np.round(totalRows * test_size) # total rows in testing dataset\n",
    "    randRowNum = np.random.randint(0, int(totalRows), int(testRows)) # randomly generated row numbers\n",
    "    testData = np.array([data[i] for i in randRowNum]) # creates test dataset\n",
    "    data = np.delete(data, randRowNum, axis = 0) # deletes test data rows from main dataset; making it training dataset\n",
    "    train_features = data[:, :-1]\n",
    "    train_target = data[:, -1].astype(int)\n",
    "    test_features = testData[:, :-1]\n",
    "    test_target = testData[:, -1].astype(int)\n",
    "    \n",
    "    return train_features, train_target, test_features, test_target    \n",
    "\n",
    "# running train_test_split for our dataset\n",
    "train_features, train_target, test_features, test_target = train_test_split(df, test_size = 0.17)\n",
    "standardScaler(train_features) # standard scaling training set \n",
    "standardScaler(test_features) # standard scaling testing set\n",
    "train_features.shape, train_target.shape, test_features.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have separate training and testing datasets, we will keep the testing dataset aside and only use it for testing purposes. All the training and optimization will be performed on the training dataset.\n",
    "Let us now go ahead and optimize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define the loss function. We will be using cross-entropy loss function for our model. For that, we'll be needing one-hot coded target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one-hot-coded target labels for training target labels\n",
    "oneHotTraining = np.array([np.zeros(5) for i in range(train_target.shape[0])])\n",
    "for label,i in zip(oneHotTraining,train_target):\n",
    "    label[int(i)] = 1\n",
    "    \n",
    "# creating one-hot-coded target labels for testing target labels\n",
    "oneHotTest = np.array([np.zeros(5) for i in range(test_target.shape[0])])\n",
    "for label,i in zip(oneHotTest,test_target):\n",
    "    label[int(i)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(probabilities, target):\n",
    "    \"\"\"Calculates cross entropy loss for a set of predictions and actual targets.\n",
    "    \n",
    "    Args-\n",
    "        predictions- Probability predictions, as returned by multinomialLogReg function\n",
    "        target- Actual target values\n",
    "    Returns- \n",
    "        CELoss- Average cross entropy loss\n",
    "    \"\"\"\n",
    "    n_samples = probabilities.shape[0]\n",
    "    CELoss = 0\n",
    "    for sample, i in zip(probabilities, target):\n",
    "        CELoss += -np.log(sample[i])\n",
    "    CELoss /= n_samples\n",
    "    return CELoss       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6600187151929244"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossEntropyLoss(probabilities, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our loss function, we will finally define our optimizer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochGradDes(learning_rate, epochs, target, features, weights, biases):\n",
    "    \"\"\"Performs stochastic gradient descent optimization on the model.\n",
    "    \n",
    "    Args-\n",
    "        learning_rate- Size of the step the function will take during optimization\n",
    "        epochs- No. of iterations the function will run for on the model\n",
    "        target- Numpy array containing actual target values\n",
    "        features- Numpy array of independent variables\n",
    "        weights- Numpy array containing weights associated with each feature\n",
    "        biases- Array containinig model biases\n",
    "    \n",
    "    Returns-\n",
    "        weights, biases, loss_list\n",
    "        where,\n",
    "            weights- Latest weight calculated (Numpy array)\n",
    "            bias- Latest bias calculated (Numpy array)\n",
    "            loss_list- Array containing list of losses observed after each epoch    \n",
    "    \"\"\"\n",
    "    target = target.astype(int)\n",
    "    loss_list = np.array([]) #initiating an empty array\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        probabilities, _ = multinomialLogReg(features, weights, biases) # Calculates probabilities for each possible outcome\n",
    "        \n",
    "        CELoss = crossEntropyLoss(probabilities, target) # Calculates cross entropy loss for actual target and predictions\n",
    "        loss_list = np.append(loss_list, CELoss) # Adds the CELoss value for the epoch to loss_list\n",
    "        \n",
    "        probabilities[np.arange(features.shape[0]),target] -= 1 # Substract 1 from the scores of the correct outcome\n",
    "        \n",
    "        grad_weight = probabilities.T.dot(features) # gradient of loss w.r.t. weights\n",
    "        grad_biases = np.sum(probabilities, axis = 0).reshape(-1,1) # gradient of loss w.r.t. biases\n",
    "        \n",
    "        #updating weights and biases\n",
    "        weights -= (learning_rate * grad_weight)\n",
    "        biases -= (learning_rate * grad_biases)\n",
    "        \n",
    "    return weights, biases, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us run the optimizer on our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedWeights, updatedBiases, loss_list = stochGradDes(0.1, 2000, train_target, train_features, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.6943323 ,  1.30404857, -4.80262456, -1.97683343, -0.92090367,\n",
       "         1.24371714, -2.34408448,  4.54045003, -3.61156067, -3.19822705,\n",
       "        -4.42638585, -5.10327903, -3.41966719],\n",
       "       [ 4.47554908, -1.20998259,  2.63898115,  2.48539   ,  3.60802298,\n",
       "        -0.52963651,  2.90350593, -4.44104526,  4.08056975,  1.35901171,\n",
       "         2.09361023,  2.20228143,  2.22310557],\n",
       "       [ 0.20405384, -1.32658598, -0.82459297, -1.03581543,  1.53944925,\n",
       "         0.97854789, -1.7283219 ,  3.66777448, -1.66084852, -1.49272627,\n",
       "        -3.42969016, -2.35354879, -2.29110342],\n",
       "       [-0.98559478, -0.38212998,  2.73590377,  0.09823772, -0.07085267,\n",
       "         0.20930983,  1.16830304, -1.99383815,  2.1119554 ,  2.32084176,\n",
       "         3.35499216,  4.08124619,  3.51331167],\n",
       "       [ 1.83092313,  3.91590813,  2.56635197,  2.248453  , -1.16100602,\n",
       "        -0.03618447,  1.89695753,  1.91415329,  1.70458123,  2.63553033,\n",
       "         3.74095162,  4.1814723 ,  1.68629001]])"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updatedWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will test our mode with the updated weights and biases. \n",
    "\n",
    "**Note- This test will be conducted on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test dataset - 63.46153846153846\n"
     ]
    }
   ],
   "source": [
    "testProbabilities, testPredictions = multinomialLogReg(test_features, updatedWeights, updatedBiases)\n",
    "\n",
    "correctPreds = 0\n",
    "for i in range(len(testPredictions)):\n",
    "    if testPredictions[i] == test_target[i]:\n",
    "        correctPreds += 1\n",
    "acc = correctPreds / len(testPredictions) * 100\n",
    "print(\"Model accuracy on test dataset - {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
